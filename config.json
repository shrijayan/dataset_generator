{   
    "inference_engine": "ollama",
    "model_name": "llama3.1",
    "model_max_tokens": 8096,
    "input_folder": "input_data",
    "output_folder": "generated_questions",
    "chroma_db_path": "chromadb",
    "chroma_collection_name": "questions",
    "duplicate_threshold": 0.1
}